---
title: "Lecture 2 - Properties"
subtitle: "<br> Econometrics 1"
author: "Vincent Bagilet"
date: "2024-09-24"
output:
  xaringan::moon_reader:
    self_contained: true
    lib_dir: libs
    css: mediocre-themer.css
    nature:
      countIncrementalSlides: no
      highlightLines: yes
      highlightStyle: github
      ratio: '16:9'
      titleSlideClass: [right, middle, inverse]
---

```{r themer, include=FALSE}
library(tidyverse)
library(knitr)
library(xaringan)
library(mediocrethemes)
library(here)
library(sf)
library(mapview)
library(plotly)

# set knit directory to "Document directory"

set.seed(12)

xaringan_mediocre(pal = "portal")
```

class: right, middle, inverse

# Quizz

---

class: right, middle, inverse

# Research questions

---

class: titled, middle

# What is a good research question?

- It **can be answered**

  - There is some sort of objective answer 

- It should **improve our understanding of the world**

  - Should inform theory in some way
  
  - Takes us from theory to an hypothesis (statement about what we will observe in the world)

---

# Start with a question

- Avoid data mining 
  
- We are interested in *why* and not *what*
  
- Data mining can still help identify *questions* to test on *other* data sets

<br>
  
# Identifying a research question
  
  - From theory
  
  - Thanks to opportunities

---

class: titled, middle

# Is your research question good? 

- **Potential results**: what would any result tell you about your theory? 

- **Feasibility**: is the right data available?

- **Scale**: how much resources would you need?

- **Research design**: is there a good one that would allow you to answer your question?

- **Keep it simple**: avoid building several questions into one

---

class: right, middle, inverse

# Summary from last week

---

class: titled, middle

# Summary from last week

- Goal: answer **research questions**

- Evaluate **theory** (there is a *why* or *because*)

- Want to describe relationships between variables

- Build an econometric **model**

- Estimate the model

- Check and interpret the results 

???

- Relationships: functional form, magnitude, sign

---

class: right, middle, inverse

# Going Further

## Repeated Regressions

---

class: titled, middle

```{r sim_data, echo=FALSE, fig.asp=0.8, fig.align='center', message=FALSE,  warning=FALSE, out.width='70%', fig.dpi=300}

baseline_param <- tibble(
  N = 1000,
  mu_educ = 3, 
  sigma_educ = 1,
  sigma_u = 8000,
  alpha = 15000,
  beta = 2000
)

#function to generate data
generate_data <- function(N,
                          mu_educ,
                          sigma_educ,
                          sigma_u, 
                          alpha,
                          beta) {
  
  data <- tibble(id = 1:N) %>%
    mutate(
      educ = rnorm(N, mu_educ, sigma_educ),
      u = rnorm(N, 0, sigma_u),
      wage = alpha + beta*educ + u
    ) 
}

select_sample <- function(df, prop_in_sample = 0.05) {
  df %>% 
    mutate(
      in_sample = purrr::rbernoulli(nrow(.), p = {{ prop_in_sample }}),
      in_sample_name = ifelse(in_sample, "In Sample", NA)
    ) 
}

population_data <- purrr::pmap(baseline_param, generate_data) |> 
  bind_rows()

plot_wage_educ <- function(df, prop_in_sample = 0.05) {
  df_sample <- df |> 
    select_sample(prop_in_sample) 
  
  df_sample |>
    ggplot(aes(x = educ, y = wage, color = in_sample_name)) +
    geom_point() +
    geom_abline(
      intercept = baseline_param$alpha, 
      slope = baseline_param$beta, 
      linewidth = 1
    ) +
    geom_smooth(
      data = df_sample |> filter(in_sample),
      method = "lm",
      formula = 'y ~ x',
      se = FALSE,
      fullrange = TRUE
    ) +
    labs(
      title = "Relationship between education and wage",
      subtitle = "Relationship in a sample",
      color = NULL,
      x = "Education (Year)",
      y = "Wage ($)"
    )
} 

plot_wage_educ(population_data, prop_in_sample = 0.05)
```

---

```{r plot_sim_2, echo=FALSE, fig.asp=0.8, fig.align='center', message=FALSE,  warning=FALSE, out.width='70%', fig.dpi=300}
plot_wage_educ(population_data, 0.05)
```

---

```{r plot_sim_3, echo=FALSE, fig.asp=0.8, fig.align='center', message=FALSE,  warning=FALSE, out.width='70%', fig.dpi=300}
plot_wage_educ(population_data, 0.05)
```

---

```{r plot_sim_4, echo=FALSE, fig.asp=0.8, fig.align='center', message=FALSE,  warning=FALSE, out.width='70%', fig.dpi=300}
plot_wage_educ(population_data, 0.05)
```

---

# Repeated regressions

- Different samples give different results 

- Let's compute a lot of regressions and store the results in a data frame

- The first results look like this:

```{r run_sim, echo=FALSE}
run_estim <- function(data) {
  data |> 
    filter(in_sample) |> 
    lm(formula = wage ~ educ) |> 
    broom::tidy(conf.int = TRUE, conf.level = 0.95) |> 
    filter(term == "educ") 
}

compute_sim <- function(data, sim_id = 1) {
  data |> 
    select_sample() |> 
    run_estim() |> 
    mutate(sim_id = sim_id)
}

n_iter <- 150

sim_results <- map(1:n_iter, .f = compute_sim, data = population_data) |> 
  bind_rows()

# saveRDS(sim_results, file = "sim_results.RDS")

sim_results |> 
  slice(1:4) |> 
  select(sim_id, estimate, std.error) |> 
  kable()
```

- Let's plot them!

---

```{r plot_estim, echo=FALSE, message=FALSE, warning=FALSE, fig.dpi=300, fig.asp=0.8, out.width='70%', fig.align='center'}
sim_results |>
  slice(1:30) |>
  ggplot(aes(x = sim_id, y = estimate)) +
  geom_point() +
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "Estimates of the parameter of interest",
    subtitle = paste("Computed on 30 different data sets"),
    x = "Simulation id",
    y = "Estimate"
  )
```

---

```{r plot_distrib_estim, echo=FALSE, message=FALSE, warning=FALSE, fig.dpi=300, fig.asp=0.8, out.width='70%', fig.align='center'}
sim_results |> 
  ggplot(aes(x = estimate)) + 
  geom_dotplot() +
  labs(
    title = paste("Distribution of", n_iter, "estimates"),
    subtitle = paste("Computed on", n_iter, "different samples"),
    x = "Estimate",
    y = NULL,
    caption = "Each dot represents one estimate"
  ) +
  theme(panel.grid.major.y = element_blank(), axis.text.y = element_blank())
```

---
class: titled, middle

# Properties of our estimator

- Are our estimates valid? 

- Are they a good approximation of the population parameters? 

```{r pop_param, echo=FALSE}
baseline_param |> 
  select(-N) |> 
  kable()
```

- We have many samples (unlike in actual settings)

- Do we, on average, retrieve the parameters of interest?

???

- Exercise

---
class: titled, middle

# Estimators as random variables

- The estimator is a **random variable** (r.v.): a variable whose outcome is uncertain

- Estimate = realization of the estimator

- For the same estimator, different samples $\Rightarrow$ different estimates

- We can however study the properties of an estimator based on one sample only

---

# Properties of our estimator

- Here, we were able to derive properties because we had many samples

- What if, like in actual settings,

--

  - We have only one draw from the population

  - Population parameters are unknown?
  
--

- We use **theoretical properties** of the estimator

- Derive conditions under wich the OLS estimator produces valid estimates

---
class: right, middle, inverse

# Statistics reviews

---
class: titled, middle

# Random variables

- **Random variable** (r.v.): a variable whose outcome is uncertain

- **Support** of a random variable: set of values the r.v. can take

- Probabilities can be assigned to the set of values in the support

  - *Example*: roll of a dice, coin flip, height of students in the class
  
---
class: titled, middle

# Probability function

- Probability that a random variable takes a given value

- Discrete variable: **probability mass function**:

  $$f_X : x \mapsto Pr[X = x]$$

- Continuous variable: **probability density function**. It is such that

  $$Pr[a \leq Z \leq b] = \int_a^b f_Z(z) \text{d}z$$
---
class: titled, middle

# Expected value

- First moment
  
- Measures the central tendency of the distribution
  
- Discrete: $\mathbb{E}[X] = \sum_{i = 1}^{s} p(X = x_i)x_i$
  
- Continuous: $\mathbb{E}[Z] = \int_{- \infty}^{+\infty} z f_Z(z) \text{d}z$

---
class: titled, middle

# Variance

- Second moment
  
- Measures the dispersion of the distribution
  
$$\text{Var} [X] = \mathbb{E}[( X âˆ’ \mu_x )^2]$$

- where $\mu_x = \mathbb{E}[X]$

- Illustrations [here](https://seeing-theory.brown.edu/basic-probability/index.html)

---
class: right, middle, inverse

# Estimator Properties

---
class: titled, middle

# Unbiasedness

- **Bias** of the estimator $\hat{\beta}$: Bias = $\mathbb{E}[\hat{\beta}|X] - \beta$

- **Unbiasedness**:

  - Bias = 0

  - Distribution of the estimator centered around the true population parameter

- If bias > 0, estimator positively biased (there is an upward bias)

---
class: titled, middle

# Efficiency

- An estimator is efficient if **its variance is smaller than that of the other comparable estimators**

- We want estimates from any sample to be close from one another

- Efficiency is **relative** and is used to compare estimators that use the same information

---
class: titled, middle

# Asymptotic Consistency

- An estimator is **consistent** if its variance decreases as the sample size increases

$$\lim_{n \to \infty} Var(\hat{\beta}| X) = 0$$

- Variance is a negative function of the sample size


---
class: titled, middle

# Asymptotic Normality

- **Asymptotic Normality**: The error follows a normal distribution with mean zero and and constant variance

$$e|X \sim \mathcal{N}(0, \sigma^2I)$$

- Necessary for hypothesis testing on the parameters and assess their generality

- The error term is the sum of all the variables that are not included in the model 

  $\to$ central limit theorem

---
class: right, middle, inverse

# Optimality

---
class: titled, middle

# Gauss-Markov theorem

- Gives the conditions under which the OLS estimator is **optimal**

- Optimal means: the unbiased linear estimator with the smallest possible variance (Best Linear Unbiased Estimator, BLUE)

- Ideal situation, often violated in practice

- Use correction and alternative estimators to recover valid estimates

---
class: titled, middle

# Linearity

- There exists a linear relationship between the inputs and the response

- The model is correctly specified

$$y = X\beta + e$$

- Mispecified model $\Rightarrow$ bias and inconsistent standard errors


---
class: titled, middle

# Exogeneity

- There is no relationship between the input and the error term

$$\mathbb{E}[e | X] = 0$$

- Also called the zero conditional mean of the error

- Causes: simultaneity, omitted variables and measurement error

---
class: titled, middle

# No perfect collinearity

- $X$ is a matrix of full rank, the $k$ columns are linearly independent

- If collinearity cannot compute the OLS estimator

- Arises when an input is a linear function of other inputs

---
class: titled, middle

# Spherical errors

- Spherical errors are a combination of: 

  - **Homoskedasticity**: the variance of the errors does not depend on $X$ ( $\mathbb{V}[e_i|X] = \sigma $ )
  
  - **No serial correlation** or **independent errors**: $e_i \perp e_j | X$ 

- Combined togehter, this gives

$$\mathbb{E}[e'e | X] = \sigma^2 I$$
---
class: titled, middle

# OLS Properties and Conditions

- Assume linearity and no perfect colinearity,

- If in addition we have

  - **Exogeneity**, the OLS estimator is **unbiased**
  
  - **Exogeneity** and **spherical errors**, the OLS estimator is **efficient** among *linear* estimators (BLUE)
  
  - That + **normally distributed errors**, the OLS estimator is **normally distributed**

---
class: right, middle, inverse

# A bit of maths

## On the board

---
class: titled, middle

# Derivations

- Bias of the estimator

- Variance of the estimator
  
---
class: right, middle, inverse

# Lecture summary

---
class: titled, middle

# This week

- Regression is a helpful tool to answer research questions

- The OLS estimator, under some conditions, has some neat properties

- We described these **properties** and some of the **necessary conditions** for these properties to hold

---
class: right, middle, inverse

# Thanks!

 





